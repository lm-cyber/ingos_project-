{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8431de45-bd61-461b-bf54-a2d2b9e41f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/void/.pyenv/versions/3.10.16/envs/venv3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "from transformers import SegformerImageProcessor#,SegformerForSemanticSegmentation\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from  transformers.models.segformer.modeling_segformer import SegformerPreTrainedModel,SegformerModel,SegformerDecodeHead\n",
    "from  transformers.modeling_outputs import BaseModelOutput, SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99266c93-b51a-4b10-b359-2a3f98ef1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# choose your loss https://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        probs = probs.clamp(min=1e-4, max=1-1e-4)\n",
    "\n",
    "        pt = (probs * targets) + ((1 - probs) * (1 - targets))\n",
    "        focal_weight = (1 - pt).pow(self.gamma)\n",
    "\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        bce_loss = F.binary_cross_entropy(probs, targets, reduction='none')\n",
    "\n",
    "        loss = alpha_t * focal_weight * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "class CombinateLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', size_average=None, reduce=None, reduction_mse='mean', coef=0.05):\n",
    "        super(CombinateLoss, self).__init__()\n",
    "        self.focal = FocalLoss(alpha, gamma, reduction)\n",
    "        self.mse = MSELoss(size_average=size_average, reduce=reduce, reduction=reduction_mse)\n",
    "        self.coef = coef\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.focal(inputs, targets)*(1-self.coef) + self.mse(inputs, targets) * self.coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf11a5c-e498-405e-83af-bb7647f349f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegformerFor–°raft(SegformerPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.segformer = SegformerModel(config)\n",
    "        self.decode_head = SegformerDecodeHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        outputs = self.segformer(\n",
    "            pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=True,  # we need the intermediate hidden states\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        encoder_hidden_states = outputs.hidden_states if return_dict else outputs[1]\n",
    "\n",
    "        logits = self.decode_head(encoder_hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.num_labels == 1:\n",
    "                raise ValueError(\"The number of labels should be greater than one\")\n",
    "            else:\n",
    "                # upsample logits to the images' original size\n",
    "                # print('l',logits.shape)\n",
    "                upsampled_logits = nn.functional.interpolate(\n",
    "                    logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "                )\n",
    "                # print('u',upsampled_logits.shape)\n",
    "\n",
    "                loss_fct = CombinateLoss()# TODO add params \n",
    "                loss = loss_fct(upsampled_logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            if output_hidden_states:\n",
    "                output = (logits,) + outputs[1:]\n",
    "            else:\n",
    "                output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=torch.clip(upsampled_logits,0,1), # interpolate loggits\n",
    "            hidden_states=outputs.hidden_states if output_hidden_states else None,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16102eea-7e2a-4fcb-9733-237d5da42601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/void/.pyenv/versions/3.10.16/envs/venv3.10/lib/python3.10/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "Some weights of SegformerFor–°raft were not initialized from the model checkpoint at nvidia/segformer-b1-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b1-finetuned-ade-512-512\")# choose differnt predtrain\n",
    "model = SegformerFor–°raft.from_pretrained( \n",
    "    \"nvidia/segformer-b1-finetuned-ade-512-512\", # choose differnt predtrain\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6daee6c-b9d1-4350-9f83-b60e9ce637a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.segformer.image_processing_segformer.SegformerImageProcessor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e474c9-22d7-41b1-bc63-9109e5e9e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.decode_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6903a48-0ca1-4911-9423-fc197722cb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2076961"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('test_data/test_heatmap.npy').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "371c7544-7622-497a-a241-d2882a9c20b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_heatmap.npy  test_img.png\n"
     ]
    }
   ],
   "source": [
    "!ls test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d18b2ab5-d194-440a-b083-d479ca097ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ExampleCRAFTDataset(Dataset):\n",
    "    def __init__(self,  feature_extractor):\n",
    "       \n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open('test_data/test_img.png').convert(\"RGB\")\n",
    "        \n",
    "      \n",
    "        mask = np.load('test_data/test_heatmap.npy')\n",
    "\n",
    "        \n",
    "\n",
    "        encoding = self.feature_extractor(\n",
    "            image,\n",
    "            size=512,\n",
    "            do_resize=True,\n",
    "            do_normalize=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        pixel_values = encoding['pixel_values'].squeeze(0)  # (3, 512, 512)\n",
    "\n",
    "\n",
    "        mask_pil_text = Image.fromarray(mask[:,:,0])  # 0-–π –∫–∞–Ω–∞–ª (Text)\n",
    "        mask_pil_link = Image.fromarray(mask[:,:,1])  # 1-–π –∫–∞–Ω–∞–ª (Link)\n",
    "        \n",
    "        mask_pil_text = mask_pil_text.resize((512, 512), resample=Image.BICUBIC)\n",
    "        mask_pil_link = mask_pil_link.resize((512, 512), resample=Image.BICUBIC)\n",
    "\n",
    "        mask_resized_text = np.array(mask_pil_text, dtype=np.float32)\n",
    "        mask_resized_link = np.array(mask_pil_link, dtype=np.float32)\n",
    "\n",
    "        mask_2ch = np.stack([mask_resized_text, mask_resized_link], axis=0)\n",
    "\n",
    "        labels = torch.from_numpy(np.clip(mask_2ch, 0, 1))  # shape (2, 512, 512)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,  # (3, 512, 512)\n",
    "            \"labels\": labels               # (2, 512, 512)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817077d7-97d9-4592-9043-8b0b1017f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ExampleCRAFTDataset(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b118c22f-2ec3-4485-bf6e-d9c9994bfaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ea3cd2-1bce-4990-b8e0-46aba538ef17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09debb3c-931e-4ea8-a616-2fef3f4096f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craft_data_collator(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])  \n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])  \n",
    "    # pixel_values: (B, 3, 512, 512)\n",
    "    # labels:       (B, 2, 512, 512)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5020c62-2d20-4df2-b89d-5f3d534a3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = {0: \"text\", 1: \"link\"}\n",
    "model.config.label2id = {\"text\": 0, \"link\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6faafa-f0c0-4734-ba30-659904717b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = {\n",
    "        \"pixel_values\":  torch.stack([test_dataset[0][\"pixel_values\"]]),\n",
    "        \"labels\":torch.stack([test_dataset[0][\"labels\"]]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a34c5f96-5c77-4bf4-834c-4e5bcf5d886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**test_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfc980de-fe63-41e2-ac72-069d833cdac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0142)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16aa694d-de1a-471c-8707-8372ad1b0285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 512, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e4b8044-8526-4531-9a75-f3300db9a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/void/.pyenv/versions/3.10.16/envs/venv3.10/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import craft_utils\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./segformer_craft\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",  # –ë—É–¥–µ–º –æ—Ü–µ–Ω–∏–≤–∞—Ç—å—Å—è –∫–∞–∂–¥—ã–π —ç–ø–æ—Ö\n",
    "    save_strategy=\"epoch\",        # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã —Ä–∞–∑ –≤ —ç–ø–æ—Ö—É\n",
    "    logging_steps=50,             # –ö–∞–∂–¥—ã–µ 50 –∏—Ç–µ—Ä–∞—Ü–∏–π –≤ –ª–æ–≥\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    # –∏ —Ç.–¥.\n",
    ")\n",
    "\n",
    "    \n",
    "\n",
    "    # coordinate adjustment\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels =eval_preds\n",
    "    # add metrics \n",
    "    # print(logits.shape)#: (batch_size, 2, H, W)\n",
    "    # print(labels.shape)#: (batch_size, 2, H, W)  (–µ—Å–ª–∏ –º—ã —Ç–∞–∫ —Ö—Ä–∞–Ω–∏–º)\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a30413b-df4a-4703-b6d7-0e4969dda8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=test_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=craft_data_collator,\n",
    "    compute_metrics=compute_metrics,  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84861746-5541-499d-b279-8572571e5197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.014263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.013152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.012963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.012984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.012849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=0.014271169900894165, metrics={'train_runtime': 5.0048, 'train_samples_per_second': 9.99, 'train_steps_per_second': 4.995, 'total_flos': 3226988917555200.0, 'train_loss': 0.014271169900894165, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingos3.10",
   "language": "python",
   "name": "ingos3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
